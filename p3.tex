\chapter{Proposal SDY} \label{ch:prop-sdy}
%This is my proposal for the work I would like to pursue over the next few years with Shahrokh's support. It relates to chapter three of this paper. 

\section{Introduction}
%topic/purpose, thesis statement

Spatial music has been a subject of great interest in the electro-acoustic domain for many decades. Ever since Schaeffer, Stockhausen, and Cage began experimenting with tape players as a means of separating the musician from the sound source, composers have ardently explored the possibilities that new technological developments afford them. The last few years have seen an explosion in the field of Extended Reality (XR) with major technology companies rolling out increasingly affordable Head-Mounted Displays (HMD) and game engines like Unity becoming easier to prototype with. 

Unfortunately, there are countless compositions being written today, that are seldom experienced with any spatial information. In our department of music, countless musical works which elegantly make use of octaphonic arrays commonly become down-mixed into stereo, destroying all the spatial information. While the ultimate stereo mix results in a high-quality standardized playback format, the option to document these works with additional spatial information warrants exploring. 

In this proposal, we wanted to explore some of the ways the existing infrastructure of our department, in addition to the vast number of options for presenting spatial audio online, could be leveraged in order to allow for modern representations of our works to be distributed, primarily in an asynchronous fashion. Traditionally, only a small number of individuals, who attend the live event, are privileged enough to listen to these musical works as intended. Our hope is to begin creating a culture of documenting and sharing our spatial works online, using free and open source software and hardware for the dissemination of these works. 

The motivation behind using Free and Open Source Software and Hardware (FOSSH) is to prevent changes in operating system to interrupt the dissemination of these works and also to lessen the financial burden on students and the University. In addition to the economic implications, we appreciate the benefit of sharing these methods with our peers in other institutions whom might be interesting in adopting any of our models to distribute spatial works. The reproducibility of computer music works relies on consistent operating systems and associated hardware, this is part of the motivation for exploring FOSSH. Finally, we believe there is a political message behind this conscious choice - it is a way of diverging from the stronghold of tech giants, relying instead on the global community of computer and electrical engineers which make all this work possible. 

\subsection{From 2D to 3D}

A common problem we encounter in multi-media arts which concern works featuring diffusion is the mismatch between video formats. A common experience for an artist is that of creating a work for a with a 2D video and 3D sound, and finding it difficult, afterwards, to represent these works with 3D audio. Alternatively, we might have spatial audio pieces which do not feature video or any other media\footnote{Another commonly used representation in multi-media works is lighting events and projection mapping parameters.}. In this case, if we wish to distribute the work using HMD technologies, we must decide what, if any, visual representation will accompany our work. 

A complete lack of sensory information might make it difficult to know if/when the piece has begun. A static picture/CGI \footnote{Computer generated imagery: virtual scenes created using frameworks such as \href{https://www.opengl.org/}{OpenGL}.} might be seen as lifeless and dull. While collaborating with visual artists might be ideal, sometimes this too can be difficult. In an unpublished project we entitled \textit{synth-esthesia} we developed a simple proof of concept using Unity to generate visual content that reacted to musical material. In the experience the user was able to mute/unmute different spatially distributed sound materials and also pick them up and move them. There was also a central hub where collision events between the controller and cubes produced melodic notes or percussive elements. 

\todo[inline]{discuss methods other than 1-to-1 matching. boring. can we do it all inside Pd?}

There remains a clear discrepancy when presenting these works in 2D. There are a few solutions to this problem:
\begin{enumerate}
    \item \textbf{Fixed-perspective 2D render}: the video is rendered in 3D but only a fixed perspective is presented live. Assumes the content of the visual material is predominantly frontal. 
    \item \textbf{Fixed-perspective 2D render}: the video is rendered in 3D but only a fixed perspective is presented live. Assumes the content of the visual material is predominantly frontal. 
    \item \textbf{Warped 2D videos}: might work for abstract material that does not necessarily rely on a clear perspective. 
    \item \textbf{2D projection in 3D}: virtual movie screen for 2D media. 
\end{enumerate}

\subsection{Interactivity}



\section{Literature Review}
%write in detail about existing solutions to the problem

\section{Methods}
%how would you like to contribute? what could be better?


\section{Results}
%you wont have any results just yet.

\section{Conclusion}
%here is why this is important